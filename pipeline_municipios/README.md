ğŸ“Š Pipeline Municipal com Airflow, PostgreSQL e DBeaver
Este projeto executa um pipeline de dados municipais, organizando informaÃ§Ãµes de populaÃ§Ã£o, leitos hospitalares e dados do ENEM em um banco PostgreSQL. O fluxo Ã© orquestrado com Airflow e os dados podem ser explorados visualmente via DBeaver.

âœ… Requisitos
Docker
Docker Compose
Python 3.10+

DBeaver Community (GUI para banco de dados)

ğŸš€ Passo a Passo
ğŸ”§ 1. Subindo os serviÃ§os com Docker
Clone o repositÃ³rio:


git clone <https://github.com/lotharmm/JetSales_.git>
cd <JetSales_>

Suba o ambiente com Docker Compose:
docker-compose up -d
Isso inicia o PostgreSQL e o Airflow (se configurado), criando o banco municipios_db.

ğŸ˜ 2. Instalando e configurando o DBeaver
ğŸ’¾ InstalaÃ§Ã£o
Baixe o DBeaver Community Edition

Instale normalmente conforme o seu sistema operacional:

ğŸ”Œ Conectando ao banco PostgreSQL do Docker
Abra o DBeaver

Clique em Database > New Database Connection

Escolha PostgreSQL

Preencha os dados da conexÃ£o conforme o docker-compose.yml:

Host: localhost

Port: 5433

Database: municipios_db

Username: airflow

Password: airflow_password

Clique em Test Connection

Se for o primeiro uso, o DBeaver pedirÃ¡ para baixar o driver do PostgreSQL. Aceite.

Clique em Finish

ğŸ‰ Agora vocÃª estÃ¡ conectado ao banco local do seu Docker via DBeaver.

ğŸ 3. Executando os scripts Python
VocÃª pode rodar cada script manualmente, com Python local, ou criando tasks no Airflow.

Requisitos locais (caso queira executar no terminal):
pip install psycopg2 pandas

âš™ï¸ Ordem de execuÃ§Ã£o (manual)
1) Download da populaÃ§Ã£o (cria CSV ou baixa dados)
python scripts/download/download_populacao.py

2) Carga da populaÃ§Ã£o no PostgreSQL
python scripts/load/load_populacao.py

3) Carga de leitos hospitalares
python scripts/load/load_leitos.py

4) Carga dos dados do ENEM
python scripts/load/load_enem.py

5) Testes de qualidade
python scripts/transform/testes_qualidade.py


Cada script realiza uma etapa especÃ­fica:
- Os arquivos load_*.py inserem dados diretamente nas tabelas do banco.
- O script testes_qualidade.py executa validaÃ§Ãµes nos dados inseridos.

ğŸ—‚ Estrutura do Projeto
pipeline_municipios/
â”œâ”€â”€ airflow/dags/
â”‚   â””â”€â”€ pipeline_municipios.py        # DAG principal do Airflow
â”œâ”€â”€ data/                             # Arquivos baixados, CSVs, etc
â”œâ”€â”€ notebooks/                        # Jupyter notebooks (opcional)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download/
â”‚   â”‚   â””â”€â”€ download_populacao.py     # Script de download
â”‚   â”œâ”€â”€ load/
â”‚   â”‚   â”œâ”€â”€ load_populacao.py
â”‚   â”‚   â”œâ”€â”€ load_leitos.py
â”‚   â”‚   â””â”€â”€ load_enem.py
â”‚   â””â”€â”€ transform/
â”‚       â””â”€â”€ testes_qualidade.py
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ modelagem.sql                 # Script para criaÃ§Ã£o de tabelas
â”œâ”€â”€ docker-compose.yml               # Infraestrutura com PostgreSQL
â””â”€â”€ README.md                        # Este guia
âœ… VerificaÃ§Ã£o Final
Todos os containers devem estar rodando:
docker ps

O DBeaver deve exibir as tabelas:
populacao
leitos
enem

ApÃ³s execuÃ§Ã£o dos scripts:

VocÃª verÃ¡ os dados nas tabelas

Testes de qualidade aparecerÃ£o no log do terminal ou console do Python